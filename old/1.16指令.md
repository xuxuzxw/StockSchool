在完成了v1.1.5的单元和集成测试后，我们验证了项目的“零件”和“组装图”是正确的。但我们还没有见证过一台完整的“发动机”——从投入“燃料”（原始数据）到输出“动力”（最终的因子和预测结果）——在真实的、连续的工况下运转的全过程。

因此，**v1.1.6作为一个“全流程实测版本”是不可或缺的**。它不是简单的重复测试，而是对整个系统在真实场景下的**稳定性、一致性和效率**的最终验收，是v1.0和v1.1所有工作的“毕业典礼”。

------


### **v1.1.6 “全流程实测”指令清单**


**核心思想**: 这不再是测试单个函数或模块，而是对整个**自动化任务流**的演练。我们将模拟一次无人干预的、从凌晨数据同步到盘后因子计算的完整日度运行，并对产出的所有中间和最终数据进行全面检验。

**【v1.1.6 指令 1/3】**

> **任务**: 创建一个全流程的调度与执行主脚本。
> 
> **背景**: 我们需要一个“总开关”来按顺序触发所有已定义的Celery自动化任务。
> 
> **当前项目已实现 `run_daily_workflow.py`**, 实际代码如下：
> 
> Python
> 
> ```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StockSchool v1.1.6 全流程日度工作流调度脚本

该脚本定义并启动一个完整的、从数据同步到预测的Celery任务链。
作者: StockSchool Team
版本: v1.1.6
创建时间: 2024-01-16
"""

import os
import sys
from celery import chain, group
import time
from celery.result import AsyncResult
from datetime import datetime
from dotenv import load_dotenv

# 加载.env文件中的环境变量
load_dotenv()

# 添加项目根目录到Python路径
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.compute.tasks import (
    app, # 导入 Celery app 实例
    sync_daily_data,
    sync_stock_data,
    calculate_daily_factors,
    calculate_stock_factors,
    train_ai_model,
    batch_prediction,
    weekly_quality_check
)
from src.utils.config_loader import Config
from loguru import logger

# 配置日志
logger.add(
    "logs/daily_workflow.log",
    rotation="1 day",
    retention="30 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}"
)

def setup_logger():
    """设置日志配置"""
    return logger

def run_full_workflow():
    """
    定义并启动一个完整的、从数据同步到预测的Celery任务链。
    包含进度跟踪、超时处理和质量检查功能。
    """
    logger.info("Starting the full daily workflow...")
    start_time = time.time()
    max_timeout = 2 * 60 * 60  # 2小时超时

    try:
        # 加载配置
        config = Config()
        # 注意：当前config.yml中没有full_test_config配置节
        # 以下参数为默认值
        start_date = '20230101'
        end_date = '20230131'
        stock_pool = ['000001.SZ', '600519.SH', '300750.SZ']
        
        logger.info(f"测试配置: {start_date} 到 {end_date}, 股票池: {stock_pool}")
        
        # 阶段1: 数据同步任务组
        logger.info("创建数据同步任务组...")
        sync_tasks = []
        for stock_code in stock_pool:
            task = sync_stock_data.s(
                ts_code=stock_code,
                start_date=start_date,
                end_date=end_date,
                test_mode=True
            )
            sync_tasks.append(task)
        
        # 并行执行数据同步
        sync_group = group(sync_tasks)
        
        # 阶段2: 因子计算任务组
        logger.info("创建因子计算任务组...")
        factor_tasks = []
        for stock_code in stock_pool:
            task = calculate_stock_factors.s(
                ts_code=stock_code,
                days=60,  # 使用60天数据计算因子
                test_mode=True
            )
            factor_tasks.append(task)
        
        # 并行执行因子计算
        factor_group = group(factor_tasks)
        
        # 阶段3: AI模型训练任务
        logger.info("创建AI模型训练任务...")
        training_task = train_ai_model.s(
            start_date=start_date,
            end_date=end_date
        )
        
        # 阶段4: 批量预测任务
        logger.info("创建批量预测任务...")
        prediction_task = batch_prediction.s(
            stock_pool=stock_pool,
            trade_date=end_date
        )
        
        # 阶段5: 质量检查任务
        logger.info("创建质量检查任务...")
        quality_check_task = weekly_quality_check.s(
            start_date=start_date,
            end_date=end_date,
            stock_pool=stock_pool
        )
        
        # 定义任务链: 数据同步 -> 因子计算 -> 模型训练 -> 批量预测 -> 质量检查
        workflow = chain(
            sync_group,
            factor_group,
            training_task,
            prediction_task,
            quality_check_task
        )
        
        # 异步执行任务链
        result = workflow.apply_async()
        logger.info(f"Workflow started with chain ID: {result.id}")
        print(f"Workflow started. Check Celery logs for progress. Chain ID: {result.id}")
        
        # 进度跟踪
        while not result.ready():
            elapsed = time.time() - start_time
            if elapsed > max_timeout:
                logger.error("Workflow timed out after 2 hours")
                return
            time.sleep(60)  # 每分钟检查一次
            logger.info(f"Workflow still running. Elapsed time: {int(elapsed/60)} minutes")
        
        if result.successful():
            logger.info("Workflow completed successfully")
            print("Workflow completed successfully")
        else:
            logger.error(f"Workflow failed: {result.result}")
            print(f"Workflow failed: {result.result}")
        
    except Exception as e:
        logger.error(f"Failed to start workflow: {e}", exc_info=True)
        print(f"Failed to start workflow: {e}")

if __name__ == '__main__':
    run_full_workflow()
```

> **完成后，请告诉我“全流程调度脚本已创建”。**

**【v1.1.6 指令 2/3】**

> **任务**: 准备实测所需的数据样本和时间范围。
> 
> **背景**: 我们需要一个足够长，又能快速跑完的真实时间段。一个月的数据是理想的选择。
> 
> **注意**: 当前项目的 `config.yml` 文件中**没有** `full_test_config` 配置节。相关参数直接在 `run_daily_workflow.py` 脚本中设置，默认值如下：
> - 开始日期: '20230101'
> - 结束日期: '20230131'
> - 股票池: ['000001.SZ', '600519.SH', '300750.SZ'] (平安银行, 贵州茅台, 宁德时代)
> 
> **当前项目的 `src/compute/tasks.py` 中已实现以下任务**: 
> 1. `sync_daily_data`: 同步每日数据，支持测试模式和进度更新
> 2. `sync_stock_data`: 同步单只股票数据，可指定日期范围和测试模式
> 3. `calculate_daily_factors`: 计算每日因子，支持测试模式和批量处理
> 4. `calculate_stock_factors`: 计算单只股票因子，支持测试模式
> 5. `weekly_quality_check`: 每周数据质量检查，包含抽样和评分
> 6. `monthly_factor_recalculation`: 每月全量因子重算
> 
> 这些任务已经能够根据参数处理指定范围和样本内的数据。
> 
> **完成后，请告诉我“实测数据样本已配置”。**

**【v1.1.6 指令 3/3】**

> **任务**: 执行全流程实测。
> 
> **背景**: 这是v1.1.6的核心交付步骤。
> 
> **请按顺序执行以下步骤**: 
> 
> 1. **清空数据库**: 运行一个脚本，清空所有核心数据表（`stock_daily`, `financial_reports`, `factor_library`, `prediction_results`等），确保我们从一个干净的状态开始。
> 2. **启动Celery Worker**: 打开一个终端，启动Celery Worker (`celery -A src.compute.tasks worker -l info -P eventlet`).
> 3. **运行工作流**: 打开另一个终端，先执行数据库初始化 `python run.py --init-db`，再运行调度脚本 `python run_daily_workflow.py`。
> 
> **请监控Celery Worker的日志输出，并将完整的日志保存下来。执行完毕后，告诉我“全流程实测已执行完毕”。**

------


### **v1.1.6 “全流程实测”质检清单 (QC)**


**质检核心思想**: 这份清单是对系统“生产能力”的第一次大考。我们需要像检查一座工厂的流水线一样，从原材料入口、中间产品到最终成品，对每个环节的数据进行严格的数量和质量检验。

| **质检环节 (Checkpoint)**     | **检验方法 (Verification Method)**                           | **验收标准 (Acceptance Criteria)**                           |
| ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1. 任务流执行一致性**       | **日志审查**: 仔细检查Celery Worker的执行日志。              | - 所有任务（数据同步、因子计算、预测、质量检查）都按照`chain`定义的**严格顺序**执行。 - 日志中没有出现任何`ERROR`或`Exception`。 - 每个任务都打印出了明确的“开始”和“成功完成”日志。 |
| **2. 输入数据完整性**         | **SQL查询**:  `SELECT COUNT(DISTINCT trade_date) FROM stock_daily WHERE ts_code IN ('000001.SZ', ...);`<br>`SELECT COUNT(*) FROM financial_reports WHERE ts_code IN (...);` | - `stock_daily`表中同步的交易日数量应与2023年1月的实际交易日数量完全一致。<br>- `financial_reports`表中应包含了样本股票在该时间段内发布的所有财报。 |
| **3. 中间产品（因子）覆盖率** | **SQL查询**:  `SELECT ts_code, COUNT(*) FROM factor_library GROUP BY ts_code;` | - 样本池中的每只股票，在每个交易日都应有对应的因子记录被生成（可能会因节假日或数据缺失有少量差异）。 - 因子值不应存在大规模的`NULL`或`NaN`。 |
| **4. 最终成品（预测）时效性** | **SQL查询**:  `SELECT MAX(trade_date) FROM prediction_results;` | - `prediction_results`表中最新的数据日期，应与我们设定的实测结束日期`2023-01-31`一致。 |
| **5. 性能与效率基线**         | **日志分析**:  分析Celery日志中每个任务的执行时间（`succeeded in Xs`）。 | - 记录下每个核心任务（数据同步、因子计算等）的**首次平均耗时**。这不必非常快，但它将是我们未来进行性能优化的**重要基准线 (Baseline)**。 |
| **6. 可重复性**               | **完全重复执行**:  再次执行**指令3/3**的所有步骤（清空数据库 -> 重新运行）。 | - 第二次运行的结果（如数据库中的最终行数、生成的因子值）应与第一次**完全一致**。 - 这证明了整个流程是确定性的、无副作用的。 |
| **7. 质量检查通过性**         | **查看质量检查报告**: 检查`weekly_quality_check`任务生成的质量报告。 | - 数据质量评分应在80分以上。 - 没有严重的数据异常或缺失。 |

完成了v1.1.6的指令和质检，您就拥有了一个**经过真实数据和全流程考验的、稳定可靠的系统核心**。这标志着您的项目已经从“开发阶段”成功迈向了“准生产阶段”，为后续更高级的策略研发和优化工作打下了最坚实的基础。
