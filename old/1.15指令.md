v1.1 “后置优化”指令清单 (复盘总结)
核心思想: 在v1.1的核心功能全部完成后，我们不应直接结束，而是需要一次“回头看”，进行代码重构、性能分析和文档沉淀，这会让系统更健壮、更高效，并为v1.2的开发铺平道路。

【v1.1 后置优化指令】

任务: 对v1.1新增的代码进行重构、性能剖析和文档化。

代码重构 (Refactoring):

审查代码: 请通读tushare_sync.py, akshare_sync.py和training_pipeline.py等v1.1新增的文件。

识别重复: 找出其中重复的代码块（例如，多个地方都出现了“从数据库读取DataFrame”的逻辑），将其抽象成一个新的通用函数，并放入src/utils/db.py中。

性能剖析 (Profiling):

定位瓶颈: 使用Python内置的cProfile工具，对factor_engine.py的run()方法和training_pipeline.py的run_training()方法进行性能分析。

执行命令:

Bash

python -m cProfile -o profile.pstats src/compute/factor_engine.py
分析结果: 请向我报告在性能分析结果中，累计耗时 (cumtime) 最长的前5个函数调用。

架构决策记录 (ADR):

创建ADR文件: 在项目根目录创建一个新目录docs/adr。

编写记录: 在该目录下创建一个新文件001-choice-of-ml-model.md，用Markdown格式记录我们为什么选择LightGBM作为初始模型（例如：因为它对表格数据效果好、训练速度快、内存占用低）。

完成后，请告诉我“v1.1后置优化已完成”。

v1.1 后置优化质检清单 (QC)
质检环节 (Checkpoint)	检验方法 (Verification Method)	验收标准 (Acceptance Criteria)
1. DRY原则 (Don't Repeat Yourself)	- 代码审查: 检查v1.1新增的代码中是否存在可以直接通过调用新创建的通用函数来替代的重复逻辑。	- 代码复用率应达到最大化，逻辑清晰，无明显的冗余代码。
2. 性能瓶颈识别	- 结果分析: 分析你提供的性能剖析报告。	- 报告必须清晰地指出了耗时最长的函数。
- 我们需要能基于这份报告，讨论出下一步的优化方向（例如：是否是数据IO过慢？某个Pandas操作效率低下？）。
3. 文档清晰度	- 人工阅读: 阅读新创建的ADR文件。	- ADR应清晰地阐述了决策的背景（Context）、决策（Decision）以及后果（Consequences），让任何不了解项目历史的人都能看懂。





整个项目中至关重要、承上启下的一环。单元测试（Unit Test）保证了每个“零件”（如`calculate_rsi`函数）是好的，但我们现在必须进行**集成测试（Integration Test）和端到端测试（End-to-End Test）**，来确保这些零件组装起来的“发动机”（数据->因子流水线）能够完美地运转，并且产出的结果是**真实、准确、可信**的。

为此，我为您设计了一套全新的，专注于**“真实数据测试”**的指令清单和质检清单。这套清单的目标是：**建立一个可重复、自动化的验证流程，来核实从API获取原始数据到最终计算出因子值的整个链路的正确性。**

------



### **v1.1.5 “真实数据测试”指令清单**



**核心思想**: 我们将在`tests`子目录下创建一个新的测试套件。这个套件不使用模拟数据（Mock），而是会**真实地调用API、读写测试数据库**，来验证一小部分真实股票在真实时间段内的完整处理流程。

**【v1.1.5 测试指令 1/3】**

> **任务**: 配置一个独立的测试数据库环境。
>
> **背景**: 为了不让测试污染我们的主开发数据库，我们需要一个专门用于自动化测试的数据库。最优雅的方式是在`docker-compose.yml`中定义一个测试数据库服务。
>
> **请修改文件 `stockschool/docker-compose.yml`**, 在`services`下添加一个新的服务`test_db`:
>
> YAML
>
> ```
> # docker-compose.yml
> # ... services: db: ... redis: ... app: ...
> ```

> test_db: image: timescale/timescaledb:latest-pg14 environment: - POSTGRES_USER=test_user - POSTGRES_PASSWORD=test_password - POSTGRES_DB=test_stockschool ports: - "5433:5432" # 映射到本地的5433端口，避免与主数据库冲突
>
> ```
> **并创建一个新的数据库连接工具 `src/utils/test_db.py`**, 用于在测试中连接到这个新数据库。
> ```

> **完成后，请告诉我“测试数据库环境已配置”。**

**【v1.1.5 测试指令 2/3】**

> **任务**: 编写一个端到端的集成测试脚本。
>
> **背景**: 这个测试将模拟一个“微缩版”的日常运行流程，但只针对一小部分我们精心挑选的数据，以确保整个流程能跑通。
>
> **请创建新文件 `stockschool/src/tests/test_integration_pipeline.py`**, 写入以下测试逻辑：
>
> Python
>
> ```
> import pytest
> import pandas as pd
> from src.data.tushare_sync import TushareSynchronizer
> from src.compute.factor_engine import FactorEngine
> from src.utils.test_db import get_test_db_engine # 使用新的测试数据库连接
> ```

> @pytest.fixture(scope="module") def setup_test_database(): # 在这里可以执行清空和初始化测试数据库表的操作 # ... yield # 在测试结束后可以执行清理操作 # ...

> def test_full_pipeline_for_sample_data(setup_test_database): """ 测试一个完整的端到端流程：同步数据 -> 计算因子 """ # 1. 定义测试样本 sample_stocks = ['000001.SZ'] # 平安银行 sample_start_date = '20230101' sample_end_date = '20230131'

> ```
> # 2. **执行数据同步**
> # 注意：需要修改TushareSynchronizer，使其可以接收一个db_engine作为参数，
> # 这样我们就可以将它指向测试数据库
> test_engine = get_test_db_engine()
> synchronizer = TushareSynchronizer(engine=test_engine)
> synchronizer.update_daily_data_for_stocks(sample_stocks, sample_start_date, sample_end_date)
> ```

> ```
> # 3. **执行因子计算**
> factor_engine = FactorEngine(engine=test_engine)
> factor_engine.run(run_date='20230131', stock_list=sample_stocks)
> ```

> ```
> # 4. **断言结果**
> # 从测试数据库中查询结果，并进行断言
> result_df = pd.read_sql(f"SELECT * FROM factor_library WHERE ts_code='000001.SZ' AND trade_date='2023-01-31'", test_engine)
> assert not result_df.empty
> assert 'rsi_14' in result_df.columns
> assert pd.notna(result_df['rsi_14'].iloc[0])
> **完成后，请告诉我“端到端集成测试已创建”。**
> ```

**【v1.1.5 测试指令 3/3】**

> **任务**: 编写一个“黄金数据”校验测试。
>
> **背景**: 我们需要对具体的因子计算结果进行精确校验。方法是，我们预先手动计算一个或几个“黄金数据”（Golden Data），然后在测试中验证系统计算的结果是否与之一致。
>
> **请在 `stockschool/src/tests/test_integration_pipeline.py` 中，添加一个新的测试函数**:
>
> Python
>
> ```
> def test_factor_golden_data_validation():
>     """
>     使用一个已知的、预先计算好的“黄金数据”来验证因子计算的精确性
>     """
>     # 1. 准备一小段精确的输入数据 (例如，从Tushare获取的真实数据，复制粘贴于此)
>     # 平安银行 2023年1月的部分收盘价
>     sample_input = pd.DataFrame({
>         'close': [14.95, 15.02, 15.01, 14.88, 14.91, 14.93, 14.98, 15.11, 15.09, 15.10, 15.15, 15.13, 15.08, 15.18]
>     })
> ```

> ```
> # 2. 预先手动或使用可信赖的工具计算出的“黄金结果”
> # 注意：这个值需要你用Excel或其他方式精确计算一次
> GOLDEN_RSI_VALUE = 63.88 # 这是一个示例值
> ```

> ```
> # 3. 调用我们的因子计算函数
> from src.compute.indicators import calculate_rsi
> calculated_rsi_series = calculate_rsi(sample_input, window=14)
> last_rsi_value = calculated_rsi_series.iloc[-1]
> ```

> ```
> # 4. 断言：系统计算结果与黄金数据是否在误差范围内一致
> assert abs(last_rsi_value - GOLDEN_RSI_VALUE) < 0.01
> **完成后，请告诉我“黄金数据校验测试已创建”。**
> ```

------



### **v1.1.5 “真实数据测试”质检清单 (QC)**



| **质检环节 (Checkpoint)** | **检验方法 (Verification Method)**                           | **验收标准 (Acceptance Criteria)**                           |
| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1. 测试环境隔离**       | **终端命令**: `docker ps`                                    | - `test_db`容器能正常启动，并监听在`5433`端口。<br>- 运行测试时，主数据库(`db`)的数据没有任何变化。 |
| **2. 端到端流程可跑通性** | **终端命令**: `pytest src/tests/test_integration_pipeline.py::test_full_pipeline_for_sample_data` | - **该测试必须能100%通过。** - 它的通过，证明了数据同步、因子计算、数据库读写这三大模块之间的接口调用和数据传递是完全正确的。 |
| **3. “黄金数据”的准确性** | **人工核验 (最关键的一步)**:  1. 亲自打开Excel或使用其他可信的在线计算器。 2. 输入`test_factor_golden_data_validation`中的`sample_input`数据。<br>3. 严格按照RSI的公式，一步步手动计算出第14天的RSI值。<br>4. 将你手动计算出的结果，替换掉代码中的`GOLDEN_RSI_VALUE`。 | - **你必须对你手动计算出的这个“黄金数据”的准确性负责。** - 它是我们验证系统计算逻辑正确性的“标准答案”。 |
| **4. 因子逻辑精确性**     | **终端命令**: `pytest src/tests/test_integration_pipeline.py::test_factor_golden_data_validation` | - **在“黄金数据”被准确无误地更新后，该测试必须能100%通过。** - 它的通过，才真正意义上证明了我们的核心算法（如RSI计算）是精确无误的。 |



这套新增的指令和质检清单，将为您的项目增加一道至关重要的“**质量安全门**”。通过了这道门，您才能充满信心地说：我的系统不仅能跑，而且跑出来的结果是**对的**。这正是从“能用”到“可靠”的决定性飞跃。
