"""
æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import os
import sys
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.acceptance.phases.performance_benchmark import PerformanceBenchmarkPhase
from src.acceptance.core.models import TestStatus

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_performance_benchmark_phase():
    """æµ‹è¯•æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µ"""
    logger.info("=== å¼€å§‹æµ‹è¯•æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µ ===")
    
    try:
        # åˆ›å»ºæ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µå®ä¾‹
        config = {
            'config_file': '.env.acceptance',
            'test_timeout': 600,  # 10åˆ†é’Ÿè¶…æ—¶
            'max_concurrent_tests': 2
        }
        
        performance_phase = PerformanceBenchmarkPhase(
            phase_name="æ€§èƒ½åŸºå‡†éªŒæ”¶æµ‹è¯•",
            config=config
        )
        
        logger.info("æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æ‰§è¡ŒéªŒæ”¶æµ‹è¯•
        logger.info("å¼€å§‹æ‰§è¡Œæ€§èƒ½åŸºå‡†éªŒæ”¶æµ‹è¯•...")
        start_time = datetime.now()
        
        results = performance_phase.execute()
        
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        logger.info(f"æ€§èƒ½åŸºå‡†éªŒæ”¶æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        # åˆ†æç»“æœ
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == TestStatus.PASSED)
        failed_tests = sum(1 for r in results if r.status == TestStatus.FAILED)
        skipped_tests = sum(1 for r in results if r.status == TestStatus.SKIPPED)
        
        logger.info(f"æµ‹è¯•ç»“æœç»Ÿè®¡:")
        logger.info(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"  é€šè¿‡æµ‹è¯•: {passed_tests}")
        logger.info(f"  å¤±è´¥æµ‹è¯•: {failed_tests}")
        logger.info(f"  è·³è¿‡æµ‹è¯•: {skipped_tests}")
        logger.info(f"  æˆåŠŸç‡: {passed_tests/total_tests:.1%}" if total_tests > 0 else "  æˆåŠŸç‡: N/A")
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        logger.info("\n=== è¯¦ç»†æµ‹è¯•ç»“æœ ===")
        for result in results:
            status_icon = {
                TestStatus.PASSED: "âœ…",
                TestStatus.FAILED: "âŒ", 
                TestStatus.SKIPPED: "â­ï¸"
            }.get(result.status, "â“")
            
            logger.info(f"{status_icon} {result.test_name}: {result.status.value} ({result.execution_time:.3f}s)")
            
            if result.error_message:
                logger.info(f"    é”™è¯¯: {result.error_message}")
            
            if result.details:
                # åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                key_details = {}
                for key, value in result.details.items():
                    if key.endswith('_score') or key.endswith('_status') or key.endswith('_acceptable'):
                        key_details[key] = value
                
                if key_details:
                    logger.info(f"    å…³é”®æŒ‡æ ‡: {key_details}")
        
        # éªŒè¯å…³é”®æµ‹è¯•é¡¹
        critical_tests = [
            'system_performance_test_suite',
            'database_performance_verification', 
            'memory_usage_test',
            'cpu_usage_monitoring_test'
        ]
        
        critical_passed = 0
        for result in results:
            if result.test_name in critical_tests and result.status == TestStatus.PASSED:
                critical_passed += 1
        
        logger.info(f"\n=== å…³é”®æµ‹è¯•é¡¹éªŒè¯ ===")
        logger.info(f"å…³é”®æµ‹è¯•é€šè¿‡: {critical_passed}/{len(critical_tests)}")
        
        if critical_passed == len(critical_tests):
            logger.info("ğŸ‰ æ‰€æœ‰å…³é”®æ€§èƒ½åŸºå‡†æµ‹è¯•é¡¹å‡é€šè¿‡ï¼")
        else:
            logger.warning("âš ï¸ éƒ¨åˆ†å…³é”®æ€§èƒ½åŸºå‡†æµ‹è¯•é¡¹æœªé€šè¿‡")
        
        # æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
        logger.info(f"\n=== æ€§èƒ½æŒ‡æ ‡æ‘˜è¦ ===")
        for result in results:
            if result.details:
                if 'performance_score' in result.details:
                    logger.info(f"{result.test_name}: æ€§èƒ½åˆ†æ•° {result.details['performance_score']}")
                elif 'memory_score' in result.details:
                    logger.info(f"{result.test_name}: å†…å­˜åˆ†æ•° {result.details['memory_score']}")
                elif 'cpu_score' in result.details:
                    logger.info(f"{result.test_name}: CPUåˆ†æ•° {result.details['cpu_score']}")
                elif 'disk_score' in result.details:
                    logger.info(f"{result.test_name}: ç£ç›˜åˆ†æ•° {result.details['disk_score']}")
        
        # æ€»ä½“è¯„ä¼°
        overall_success = passed_tests >= total_tests * 0.8  # 80%é€šè¿‡ç‡
        
        if overall_success:
            logger.info("ğŸ‰ æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µæµ‹è¯•æ€»ä½“æˆåŠŸï¼")
        else:
            logger.warning("âš ï¸ æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µæµ‹è¯•éœ€è¦æ”¹è¿›")
        
        return results
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åŸºå‡†éªŒæ”¶é˜¶æ®µæµ‹è¯•å¤±è´¥: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    try:
        results = test_performance_benchmark_phase()
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == TestStatus.PASSED)
        
        if passed_tests >= total_tests * 0.8:  # 80%é€šè¿‡ç‡
            sys.exit(0)  # æˆåŠŸ
        else:
            sys.exit(1)  # å¤±è´¥
            
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(2)  # é”™è¯¯


if __name__ == "__main__":
    main()