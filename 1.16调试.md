我们已经制定了“如何构建”和“如何质检”的蓝图，但缺少了一份同样重要的文档：**《系统日常运行与调试手册 (Operations & Debugging Playbook)》**。

这份手册的目标，是让任何一个操作者（无论是您自己还是AI）都能在没有我的情况下，独立、自信地完成从v1.0到v1.1.6所有功能的日常运行、状态监控和故障排查。它不再是关于“编码”，而是关于“**操作**”。

这是一份将所有零散指令串联起来的、可执行的“**总纲领**”。我为您精心设计了这份手册。

------



### **《StockSchool v1.1.6 - 运维与调试总指令手册》**



**核心思想**: 这份手册模拟了一位资深运维工程师的日常工作。我们假设系统已经构建完毕，现在需要每天稳定地运行它。手册将分为四个核心部分：**飞行前检查、日常飞行、飞行后分析**以及**紧急情况处理**。

------



### **第一部分：飞行前检查 (Pre-Flight Checklist)**



**目标**: 在启动任何任务之前，确保系统环境和外部依赖是健康的。

**【运维指令 1/8】**

> **任务**: 环境健康检查。
>
> **请按顺序执行以下检查，并报告每一项的状态（OK / FAILED）**:
>
> 1. **Docker服务检查**:
>
>    - **命令**: `docker ps`
>    - **预期**: `db`, `test_db`, `redis`三个容器都处于`running`状态。
>
> 2. **环境变量检查**:
>
>    - **命令**: `cat .env` (或在Python中加载并打印)
>    - **预期**: `TUSHARE_TOKEN`, `DB_PASSWORD`等关键变量都已正确填写，没有保留`YOUR_...`等占位符。
>
> 3. **主数据库连接测试**:
>
>    - **命令**: `psql "postgresql://user:${DB_PASSWORD}@localhost:5432/stockschool" -c "\dt"`
>    - **预期**: 成功连接并列出所有数据表，没有任何连接错误。
>
> 4. **Tushare API连通性测试**:
>
>    - **执行Python脚本**:
>
>      Python
>
>      ```
>      import tushare as ts, os
>      from dotenv import load_dotenv
>      load_dotenv()
>      pro = ts.pro_api(os.getenv('TUSHARE_TOKEN'))
>      df = pro.trade_cal(exchange='', start_date='20230101', end_date='20230101')
>      print("Tushare API connection successful.")
>      ```
>
>    - **预期**: 脚本正常运行，不抛出任何关于Token无效或网络超时的异常。

------



### **第二部分：日常飞行 (Daily Operations Workflow)**



**目标**: 执行完整的日度数据处理流水线。

**【运维指令 2/8】**

> **任务**: 启动Celery监控。
>
> **在一个独立的终端窗口中，启动Celery Worker，并让它持续运行，用于观察后续任务**:
>
> Bash
>
> ```
> # 激活虚拟环境
> .\venv\Scripts\activate
> # 启动Celery Worker
> celery -A src.compute.celery_app worker -l info -P eventlet
> ```
>
> **请将这个终端窗口保持在前台，我们将用它来监控整个流程的执行日志。**

**【运维指令 3/8】**

> **任务**: 触发完整的日度工作流。
>
> **打开一个新的终端窗口，执行我们v1.1.6创建的全流程调度脚本**:
>
> Bash
>
> ```
> # 激活虚拟环境
> .\venv\Scripts\activate
> # 执行主工作流
> python run_daily_workflow.py
> ```
>
> **请观察上一步的Celery Worker日志，并向我报告工作流是否已成功启动，以及任务链ID。**

**【运维指令 4/8】**

> **任务**: 飞行过程中的数据质量检验 (飞行中质检)。
>
> **在工作流运行期间或刚刚结束后，执行以下SQL查询，对中间数据进行快速抽查**:
>
> 1. **检查数据同步量**:
>
>    SQL
>
>    ```
>    -- 查询最新同步的日线数据行数
>    SELECT COUNT(*) FROM stock_daily WHERE trade_date = (SELECT MAX(trade_date) FROM stock_daily);
>    ```
>
>    - **预期**: 行数应在4000-6000之间，符合A股上市公司的数量级。
>
> 2. **检查因子生成覆盖率**:
>
>    SQL
>
>    ```
>    -- 查询最新一天生成的因子数量
>    SELECT COUNT(*) FROM factor_library WHERE trade_date = (SELECT MAX(trade_date) FROM factor_library);
>    ```
>
>    - **预期**: 数量应与上面查到的日线数据行数大致相等。
>
> **请向我报告这两项查询的结果。**

------



### **第三部分：飞行后分析 (Post-Flight Analysis)**



**目标**: 对当天的最终产出结果进行全面、深入的分析和验证。

**【运维指令 5/8】**

> **任务**: 最终预测结果检验。
>
> **执行以下SQL查询，深入分析`prediction_results`表**:
>
> 1. **检查结果时效性**: `SELECT MAX(trade_date) FROM prediction_results;`
> 2. **检查分数分布**: `SELECT MIN(score), MAX(score), AVG(score), COUNT(*) FROM prediction_results WHERE trade_date = (SELECT MAX(trade_date));`
> 3. **抽查明星股**: `SELECT ts_code, name, score FROM prediction_results WHERE trade_date = (SELECT MAX(trade_date)) ORDER BY score DESC LIMIT 10;`
>
> **请将这三项查询的结果整理成一个简短的报告给我。**

**【运维指令 6/8】**

> **任务**: 因子有效性初步分析。
>
> **这是一个高级分析任务。请执行一个Python脚本，计算最新一天某个核心因子（如RSI_14）与未来一日收益率的皮尔逊相关系数 (Pearson Correlation)**:
>
> Python
>
> ```
> # 这是一个简化的逻辑，你需要编写一个完整的脚本
> # 1. 从factor_library获取最新一天的RSI_14因子
> # 2. 从stock_daily获取对应的未来一日收益率
> # 3. 合并两个DataFrame
> # 4. 计算相关系数: merged_df['rsi_14'].corr(merged_df['future_return'])
> ```
>
> **请向我报告计算出的相关系数。这个值不必很高，但它为我们评估因子有效性提供了第一个量化指标。**

------



### **第四部分：紧急情况处理 (Emergency & Troubleshooting)**



**目标**: 当出现问题时，提供标准化的诊断和解决流程。

**【运维指令 7/8】**

> **任务**: 诊断数据同步失败。
>
> 情景: Celery日志显示sync_daily_data_task任务失败。
>
> 请按以下步骤进行故障排查:
>
> 1. **查看错误日志**: 在Celery Worker的日志中，找到具体的`ERROR`信息和Traceback。
> 2. **隔离问题**: 判断是Tushare API问题还是数据库写入问题。
> 3. **手动重跑**: **只**运行`src/data/tushare_sync.py`脚本，看是否能复现错误。
> 4. **常见原因**:
>    - **Tushare问题**: Token过期？积分不足？Tushare服务器临时故障？
>    - **数据库问题**: 密码错误？磁盘空间不足？网络连接中断？
>
> **请根据你的排查结果，告诉我最可能的原因是什么。**

**【运维指令 8/8】**

> **任务**: 重新运行失败的工作流。
>
> 情景: 解决了数据同步问题后，我们需要从失败的步骤继续。
>
> 请执行以下操作:
>
> 1. **数据清理**: 如果失败的任务造成了部分数据写入（脏数据），需要先手动执行SQL`DELETE`语句，删除当天已写入的不完整数据。
>
> 2. **选择性重跑**: 与其重新运行整个工作流，不如只调用失败的那个Celery任务。例如：
>
>    Python
>
>    ```
>    from src.compute.tasks import sync_daily_data_task
>    sync_daily_data_task.apply_async()
>    ```
>
> 3. **确认状态**: 在任务成功后，检查数据库，确认数据已补全。
>
> **请告诉我你将如何执行数据清理和任务重跑。**

这份运维与调试总指令手册，为您提供了一套完整的、可操作的、闭环的“**SOP（标准作业程序）**”。它确保了无论操作者是谁，都能以同样专业、严谨的方式来运行和维护您的StockSchool系统，保证其长期稳定、可靠地运行。
