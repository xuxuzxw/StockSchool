# 审查报告：user-interface-system

## 1. 整体评估

用户界面系统（user-interface-system）的核心是一个基于 `Streamlit` 的模型解释器仪表盘，代码位于 `src/ui/explainer_dashboard.py`。该仪表盘功能强大、交互性强，为模型的透明度和可解释性提供了有力的支持。它允许用户通过Web界面直观地理解模型行为，而无需编写代码。

**核心优势**：
- **功能丰富**：支持模型信息查看、多种特征重要性计算方法（默认、排列、SHAP）、单样本预测解释、批量解释和多模型对比分析。
- **交互性强**：利用 `Streamlit` 的特性，提供了文件上传、下拉选择、滑块等多种交互组件，用户体验良好。
- **代码结构清晰**：UI代码与模型解释的核心逻辑（`src/strategy/model_explainer.py`）分离，`explainer_dashboard.py` 专注于界面展示和用户交互，而 `model_explainer.py` 负责具体的计算任务，符合良好的软件设计原则。
- **可视化效果好**：使用 `Matplotlib` 和 `Seaborn` 生成了清晰直观的图表，如特征重要性条形图、SHAP贡献图等，有效提升了模型解释的可视化水平。

**潜在问题与欠缺**：
- **独立性过强**：仪表盘是一个完全独立的工具，依赖用户手动上传模型（`.pkl`/`.joblib`）和数据（`.csv`）文件。它与项目的数据管理、模型训练和存储等核心流程没有集成，导致工作流断裂。
- **文档与实现不一致**：`user_manual.md` 中提到的 `run_explainer_api.py` 脚本在项目中不存在。实际的启动方式是 `streamlit run src/ui/explainer_dashboard.py`。这种不一致会给新用户带来困惑。
- **缺乏统一的UI入口**：目前仅有一个模型解释器仪表盘，项目缺乏一个统一的、可以访问和管理其他核心功能（如数据同步、因子计算、模型训练）的用户界面。这使得整个系统更像一个后台工具集，而非一个完整的、用户友好的投研平台。
- **配置硬编码**：虽然项目有统一的配置加载机制，但仪表盘本身并未充分利用，例如，没有提供从配置中加载默认模型或数据路径的选项。

## 2. 检查调试内容

- **[BUG] 文档与代码不一致**：
  - **问题描述**：`user_manual.md` 描述的启动方式 `python run_explainer_api.py` 是错误的。
  - **验证方法**：在项目根目录下查找 `run_explainer_api.py` 文件，确认其不存在。尝试使用 `streamlit run src/ui/explainer_dashboard.py` 命令，确认可以成功启动仪表盘。

- **[WARNING] 工作流集成缺失**：
  - **问题描述**：仪表盘无法自动加载项目中已经训练好的模型或准备好的数据集，需要用户手动导出再上传。
  - **验证方法**：审查 `explainer_dashboard.py` 代码，确认其数据和模型的来源仅为 `st.file_uploader`，没有从项目内部路径加载的逻辑。

- **[INFO] 依赖关系确认**：
  - **问题描述**：需要确认仪表盘正常运行所需的核心依赖。
  - **验证方法**：分析 `explainer_dashboard.py` 的 `import` 语句，确认其依赖于 `streamlit`、`pandas`、`matplotlib`、`seaborn`、`joblib` 以及项目内部的 `src.strategy.model_explainer` 和 `src.utils.config_loader`。

## 3. 修改建议

- **[MUST] 修正文档**：
  - **建议**：更新 `user_manual.md`，将启动模型解释功能的命令修改为 `streamlit run src/ui/explainer_dashboard.py`，并删除对 `run_explainer_api.py` 的引用。

- **[SHOULD] 集成模型和数据加载**：
  - **建议**：在仪表盘的侧边栏增加一个选项，允许用户从项目的模型存储目录（例如 `models/`）和数据目录中选择已有的模型和数据集进行加载。这可以极大地提升工作效率，形成闭环的工作流。
  - **实现思路**：使用 `os.listdir` 或 `glob` 扫描指定目录下的文件，并将其作为 `st.selectbox` 的选项。选择后，直接使用 `joblib.load` 或 `pd.read_csv` 加载。

- **[COULD] 开发统一的Web UI**：
  - **建议**：考虑使用 `Streamlit` 或 `Dash` 构建一个更高层次的Web应用，作为整个 `StockSchool` 项目的统一入口。该UI可以包含多个页面，分别对应数据管理、因子计算、模型训练、回测和模型解释等功能。
  - **实现思路**：创建一个主应用文件（如 `app.py`），通过侧边栏或导航栏链接到不同的功能模块。`explainer_dashboard.py` 可以被重构为一个子模块，被主应用调用。

- **[COULD] 增加配置选项**：
  - **建议**：通过配置文件 `config.yml` 为仪表盘提供默认路径，例如默认的模型路径和数据路径，减少用户重复操作。

## 4. 验收调试方法

- **功能验收**：
  1. 启动仪表盘：`streamlit run src/ui/explainer_dashboard.py`。
  2. **模型信息**：上传一个 `.pkl` 模型文件，验证是否能正确显示模型类型等信息。
  3. **特征重要性**：上传模型和数据，分别测试 `default`、`permutation` 和 `shap` 方法，验证是否能生成特征重要性表格和图表。
  4. **预测解释**：选择一个样本，验证是否能显示预测值、基础值，并生成SHAP贡献图。
  5. **批量解释**：选择一个样本范围，验证是否能计算平均SHAP值并展示图表，同时提供每个样本的详细解释。
  6. **模型对比**：上传至少两个模型，验证是否能生成特征重要性和预测值的对比图表。

- **异常调试**：
  1. **文件上传失败**：上传一个非 `pkl` 或 `csv` 格式的文件，验证应用是否会给出明确的错误提示。
  2. **数据格式不匹配**：上传一个特征列与模型训练时不一致的数据文件，验证在计算重要性或解释时是否会抛出可读的错误信息。
  3. **计算错误**：对于不支持特定解释方法（如 `feature_importances_`）的模型，验证仪表盘是否能优雅地处理这种情况，而不是直接崩溃。