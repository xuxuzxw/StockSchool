#!/usr/bin/env python3
"""
ç›‘æ§ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ã€ç¼“å­˜æ•ˆç‡ã€WebSocketä¼ è¾“æ€§èƒ½ç­‰
æä¾›æ€§èƒ½åŸºå‡†æ•°æ®å’Œä¼˜åŒ–å»ºè®®

ä½œè€…: StockSchool Team
åˆ›å»ºæ—¶é—´: 2025-01-02
"""

import asyncio
import time
import statistics
import logging
import json
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import psutil
import gc

# å¯¼å…¥æµ‹è¯•ä¾èµ–
try:
    import pytest
    import pytest_asyncio
    from unittest.mock import Mock, AsyncMock
except ImportError as e:
    logging.warning(f"æµ‹è¯•ä¾èµ–ä¸å¯ç”¨: {e}")
    pytest = None

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from src.services.monitoring_service import (
        MonitoringService, CacheManager, MetricsStorage, CacheConfig, StorageConfig
    )
    from src.database.performance_optimization import DatabaseOptimizer, QueryOptimizer
    from src.websocket.monitoring_websocket import (
        MonitoringWebSocketServer, ConnectionManager, WebSocketMessage, MessageType
    )
    from src.schemas.monitoring_schemas import MonitoringMetricSchema, MetricType
except ImportError as e:
    logging.warning(f"å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {e}")
    # åˆ›å»ºæ¨¡æ‹Ÿç±»ç”¨äºæµ‹è¯•
    class MonitoringService:
        async def initialize(self): pass
        async def close(self): pass
    
    class CacheManager:
        def __init__(self, config): pass
        async def initialize(self): pass
        async def close(self): pass
    
    class DatabaseOptimizer:
        def __init__(self, url): pass
        async def initialize(self): pass
        async def close(self): pass

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    duration_seconds: float
    operations_count: int
    ops_per_second: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success_rate: float
    error_count: int
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    min_time: float
    max_time: float
    avg_time: float
    median_time: float
    p95_time: float
    p99_time: float
    std_dev: float
    
    @classmethod
    def from_times(cls, times: List[float]) -> 'PerformanceMetrics':
        """ä»æ—¶é—´åˆ—è¡¨åˆ›å»ºæ€§èƒ½æŒ‡æ ‡"""
        if not times:
            return cls(0, 0, 0, 0, 0, 0, 0)
        
        sorted_times = sorted(times)
        return cls(
            min_time=min(times),
            max_time=max(times),
            avg_time=statistics.mean(times),
            median_time=statistics.median(times),
            p95_time=sorted_times[int(len(sorted_times) * 0.95)],
            p99_time=sorted_times[int(len(sorted_times) * 0.99)],
            std_dev=statistics.stdev(times) if len(times) > 1 else 0
        )


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•åŸºç±»"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"{__name__}.{name}")
        self.results: List[BenchmarkResult] = []
        
    async def setup(self) -> None:
        """æµ‹è¯•å‰è®¾ç½®"""
        pass
    
    async def teardown(self) -> None:
        """æµ‹è¯•åæ¸…ç†"""
        pass
    
    async def run_benchmark(self, 
                          test_func: Callable,
                          iterations: int = 100,
                          warmup_iterations: int = 10,
                          **kwargs) -> BenchmarkResult:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        # é¢„çƒ­
        self.logger.info(f"å¼€å§‹é¢„çƒ­ {warmup_iterations} æ¬¡...")
        for _ in range(warmup_iterations):
            try:
                await test_func(**kwargs)
            except Exception as e:
                self.logger.warning(f"é¢„çƒ­å¤±è´¥: {e}")
        
        # åƒåœ¾å›æ”¶
        gc.collect()
        
        # è®°å½•åˆå§‹çŠ¶æ€
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        self.logger.info(f"å¼€å§‹åŸºå‡†æµ‹è¯• {iterations} æ¬¡...")
        times = []
        errors = 0
        start_time = time.time()
        
        for i in range(iterations):
            try:
                operation_start = time.perf_counter()
                await test_func(**kwargs)
                operation_end = time.perf_counter()
                times.append(operation_end - operation_start)
                
            except Exception as e:
                errors += 1
                self.logger.error(f"æµ‹è¯•è¿­ä»£ {i} å¤±è´¥: {e}")
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # è®°å½•æœ€ç»ˆçŠ¶æ€
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        cpu_percent = process.cpu_percent()
        
        # è®¡ç®—ç»“æœ
        success_count = len(times)
        ops_per_second = success_count / total_duration if total_duration > 0 else 0
        success_rate = success_count / iterations if iterations > 0 else 0
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = BenchmarkResult(
            test_name=self.name,
            duration_seconds=total_duration,
            operations_count=success_count,
            ops_per_second=ops_per_second,
            memory_usage_mb=final_memory - initial_memory,
            cpu_usage_percent=cpu_percent,
            success_rate=success_rate,
            error_count=errors,
            metadata={
                'performance_metrics': PerformanceMetrics.from_times(times).__dict__ if times else {},
                'iterations': iterations,
                'warmup_iterations': warmup_iterations,
                'initial_memory_mb': initial_memory,
                'final_memory_mb': final_memory
            }
        )
        
        self.results.append(result)
        return result


class DatabaseBenchmark(PerformanceBenchmark):
    """æ•°æ®åº“æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, database_url: str):
        super().__init__("DatabaseBenchmark")
        self.database_url = database_url
        self.optimizer: Optional[DatabaseOptimizer] = None
        self.query_optimizer: Optional[QueryOptimizer] = None
    
    async def setup(self) -> None:
        """è®¾ç½®æ•°æ®åº“è¿æ¥"""
        try:
            self.optimizer = DatabaseOptimizer(self.database_url)
            await self.optimizer.initialize()
            self.query_optimizer = QueryOptimizer(self.optimizer)
            self.logger.info("æ•°æ®åº“åŸºå‡†æµ‹è¯•è®¾ç½®å®Œæˆ")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")
            # ä½¿ç”¨æ¨¡æ‹Ÿå¯¹è±¡
            self.optimizer = Mock()
            self.query_optimizer = Mock()
    
    async def teardown(self) -> None:
        """æ¸…ç†æ•°æ®åº“è¿æ¥"""
        if self.optimizer and hasattr(self.optimizer, 'close'):
            await self.optimizer.close()
    
    async def test_query_performance(self, query_name: str, params: Dict[str, Any]) -> None:
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        if not self.query_optimizer:
            return
        
        try:
            results, metrics = await self.query_optimizer.execute_optimized_query(
                query_name, params
            )
            return len(results)
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def test_index_creation(self) -> None:
        """æµ‹è¯•ç´¢å¼•åˆ›å»ºæ€§èƒ½"""
        if not self.optimizer:
            return
        
        try:
            results = await self.optimizer.create_optimization_indexes()
            return len(results.get('created', []))
        except Exception as e:
            self.logger.error(f"ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    async def run_database_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰æ•°æ®åº“åŸºå‡†æµ‹è¯•"""
        await self.setup()
        
        try:
            results = []
            
            # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
            query_params = {
                'start_time': datetime.now() - timedelta(hours=1),
                'end_time': datetime.now(),
                'metric_names': ['cpu_usage', 'memory_usage'],
                'limit': 1000
            }
            
            result = await self.run_benchmark(
                self.test_query_performance,
                iterations=50,
                query_name='get_recent_metrics',
                params=query_params
            )
            results.append(result)
            
            # æµ‹è¯•ç´¢å¼•åˆ›å»ºæ€§èƒ½
            result = await self.run_benchmark(
                self.test_index_creation,
                iterations=1,  # ç´¢å¼•åˆ›å»ºåªéœ€è¦æµ‹è¯•ä¸€æ¬¡
                warmup_iterations=0
            )
            results.append(result)
            
            return results
            
        finally:
            await self.teardown()


class CacheBenchmark(PerformanceBenchmark):
    """ç¼“å­˜æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, cache_config: Optional[CacheConfig] = None):
        super().__init__("CacheBenchmark")
        self.cache_config = cache_config or CacheConfig()
        self.cache_manager: Optional[CacheManager] = None
    
    async def setup(self) -> None:
        """è®¾ç½®ç¼“å­˜ç®¡ç†å™¨"""
        try:
            self.cache_manager = CacheManager(self.cache_config)
            await self.cache_manager.initialize()
            self.logger.info("ç¼“å­˜åŸºå‡†æµ‹è¯•è®¾ç½®å®Œæˆ")
        except Exception as e:
            self.logger.error(f"ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
            self.cache_manager = Mock()
    
    async def teardown(self) -> None:
        """æ¸…ç†ç¼“å­˜è¿æ¥"""
        if self.cache_manager and hasattr(self.cache_manager, 'close'):
            await self.cache_manager.close()
    
    async def test_cache_set(self, key: str, value: Any) -> None:
        """æµ‹è¯•ç¼“å­˜è®¾ç½®æ€§èƒ½"""
        if not self.cache_manager:
            return
        
        await self.cache_manager.set(key, value, 300)
    
    async def test_cache_get(self, key: str) -> Any:
        """æµ‹è¯•ç¼“å­˜è·å–æ€§èƒ½"""
        if not self.cache_manager:
            return None
        
        return await self.cache_manager.get(key)
    
    async def test_batch_operations(self, data: Dict[str, Any]) -> None:
        """æµ‹è¯•æ‰¹é‡æ“ä½œæ€§èƒ½"""
        if not self.cache_manager:
            return
        
        # æ‰¹é‡è®¾ç½®
        await self.cache_manager.batch_set(data, 300)
        
        # æ‰¹é‡è·å–
        keys = list(data.keys())
        results = await self.cache_manager.batch_get(keys)
        return len(results)
    
    async def run_cache_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰ç¼“å­˜åŸºå‡†æµ‹è¯•"""
        await self.setup()
        
        try:
            results = []
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_data = {
                f"test_key_{i}": {
                    "timestamp": datetime.now().isoformat(),
                    "value": random.random(),
                    "metadata": {"index": i}
                }
                for i in range(100)
            }
            
            # æµ‹è¯•å•ä¸ªè®¾ç½®æ€§èƒ½
            result = await self.run_benchmark(
                self.test_cache_set,
                iterations=1000,
                key="benchmark_key",
                value={"test": "data", "timestamp": datetime.now().isoformat()}
            )
            results.append(result)
            
            # é¢„è®¾ä¸€äº›æ•°æ®ç”¨äºè·å–æµ‹è¯•
            for key, value in list(test_data.items())[:10]:
                await self.test_cache_set(key, value)
            
            # æµ‹è¯•å•ä¸ªè·å–æ€§èƒ½
            result = await self.run_benchmark(
                self.test_cache_get,
                iterations=1000,
                key="test_key_0"
            )
            results.append(result)
            
            # æµ‹è¯•æ‰¹é‡æ“ä½œæ€§èƒ½
            result = await self.run_benchmark(
                self.test_batch_operations,
                iterations=100,
                data=test_data
            )
            results.append(result)
            
            return results
            
        finally:
            await self.teardown()


class WebSocketBenchmark(PerformanceBenchmark):
    """WebSocketæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        super().__init__("WebSocketBenchmark")
        self.websocket_server: Optional[MonitoringWebSocketServer] = None
        self.connection_manager: Optional[ConnectionManager] = None
    
    async def setup(self) -> None:
        """è®¾ç½®WebSocketæœåŠ¡å™¨"""
        try:
            self.websocket_server = MonitoringWebSocketServer()
            await self.websocket_server.start()
            self.connection_manager = self.websocket_server.connection_manager
            self.logger.info("WebSocketåŸºå‡†æµ‹è¯•è®¾ç½®å®Œæˆ")
        except Exception as e:
            self.logger.error(f"WebSocketè®¾ç½®å¤±è´¥: {e}")
            self.websocket_server = Mock()
            self.connection_manager = Mock()
    
    async def teardown(self) -> None:
        """æ¸…ç†WebSocketæœåŠ¡å™¨"""
        if self.websocket_server and hasattr(self.websocket_server, 'stop'):
            await self.websocket_server.stop()
    
    async def test_message_compression(self, message_size: int) -> None:
        """æµ‹è¯•æ¶ˆæ¯å‹ç¼©æ€§èƒ½"""
        # åˆ›å»ºå¤§æ¶ˆæ¯
        large_data = {
            "type": "test_data",
            "payload": "x" * message_size,
            "timestamp": datetime.now().isoformat(),
            "metadata": {"size": message_size}
        }
        
        message = WebSocketMessage(
            type=MessageType.DATA,
            data=large_data
        )
        
        # æµ‹è¯•å‹ç¼©
        compressed_message = message.compress(1024)
        
        # å¦‚æœæ¶ˆæ¯è¢«å‹ç¼©ï¼Œæµ‹è¯•è§£å‹ç¼©
        if compressed_message.compressed:
            decompressed_message = WebSocketMessage.decompress(compressed_message)
            return len(json.dumps(decompressed_message.to_dict()))
        
        return len(json.dumps(message.to_dict()))
    
    async def test_batch_messaging(self, batch_size: int) -> None:
        """æµ‹è¯•æ‰¹é‡æ¶ˆæ¯æ€§èƒ½"""
        if not self.connection_manager:
            return
        
        # åˆ›å»ºæ¨¡æ‹Ÿè¿æ¥
        mock_websocket = Mock()
        mock_websocket.send_text = AsyncMock()
        
        from src.websocket.monitoring_websocket import ClientConnection, SubscriptionType
        
        connection = ClientConnection(
            client_id="test_client",
            websocket=mock_websocket,
            subscriptions={SubscriptionType.SYSTEM_HEALTH},
            connected_at=datetime.now(),
            last_ping=datetime.now(),
            metadata={},
            supports_batching=True,
            batch_size=batch_size
        )
        
        # åˆ›å»ºæ‰¹é‡æ¶ˆæ¯
        messages = []
        for i in range(batch_size):
            message = WebSocketMessage(
                type=MessageType.DATA,
                data={"index": i, "timestamp": datetime.now().isoformat()}
            )
            messages.append(message)
        
        # æµ‹è¯•æ‰¹é‡å‘é€
        await self.connection_manager._send_batch_messages(connection, messages)
        return batch_size
    
    async def run_websocket_benchmarks(self) -> List[BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰WebSocketåŸºå‡†æµ‹è¯•"""
        await self.setup()
        
        try:
            results = []
            
            # æµ‹è¯•æ¶ˆæ¯å‹ç¼©æ€§èƒ½
            result = await self.run_benchmark(
                self.test_message_compression,
                iterations=100,
                message_size=5000  # 5KBæ¶ˆæ¯
            )
            results.append(result)
            
            # æµ‹è¯•æ‰¹é‡æ¶ˆæ¯æ€§èƒ½
            result = await self.run_benchmark(
                self.test_batch_messaging,
                iterations=50,
                batch_size=20
            )
            results.append(result)
            
            return results
            
        finally:
            await self.teardown()


class IntegratedBenchmark:
    """é›†æˆæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, 
                 database_url: str = "postgresql://stockschool:stockschool123@localhost:15432/stockschool",
                 cache_config: Optional[CacheConfig] = None):
        self.database_url = database_url
        self.cache_config = cache_config or CacheConfig()
        self.logger = logging.getLogger(f"{__name__}.IntegratedBenchmark")
        self.results: Dict[str, List[BenchmarkResult]] = {}
    
    async def run_all_benchmarks(self) -> Dict[str, List[BenchmarkResult]]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # æ•°æ®åº“åŸºå‡†æµ‹è¯•
        self.logger.info("ğŸ“Š è¿è¡Œæ•°æ®åº“åŸºå‡†æµ‹è¯•...")
        db_benchmark = DatabaseBenchmark(self.database_url)
        self.results['database'] = await db_benchmark.run_database_benchmarks()
        
        # ç¼“å­˜åŸºå‡†æµ‹è¯•
        self.logger.info("ğŸ—„ï¸ è¿è¡Œç¼“å­˜åŸºå‡†æµ‹è¯•...")
        cache_benchmark = CacheBenchmark(self.cache_config)
        self.results['cache'] = await cache_benchmark.run_cache_benchmarks()
        
        # WebSocketåŸºå‡†æµ‹è¯•
        self.logger.info("ğŸ”Œ è¿è¡ŒWebSocketåŸºå‡†æµ‹è¯•...")
        ws_benchmark = WebSocketBenchmark()
        self.results['websocket'] = await ws_benchmark.run_websocket_benchmarks()
        
        self.logger.info("âœ… æ‰€æœ‰åŸºå‡†æµ‹è¯•å®Œæˆ")
        return self.results
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {},
            "detailed_results": {},
            "recommendations": []
        }
        
        # æ±‡æ€»ç»“æœ
        total_tests = 0
        total_ops = 0
        avg_ops_per_second = 0
        
        for category, results in self.results.items():
            category_summary = {
                "test_count": len(results),
                "total_operations": sum(r.operations_count for r in results),
                "avg_ops_per_second": statistics.mean([r.ops_per_second for r in results]) if results else 0,
                "avg_success_rate": statistics.mean([r.success_rate for r in results]) if results else 0,
                "total_errors": sum(r.error_count for r in results)
            }
            
            report["summary"][category] = category_summary
            report["detailed_results"][category] = [r.to_dict() for r in results]
            
            total_tests += len(results)
            total_ops += category_summary["total_operations"]
            avg_ops_per_second += category_summary["avg_ops_per_second"]
        
        # æ€»ä½“æ‘˜è¦
        report["summary"]["overall"] = {
            "total_test_categories": len(self.results),
            "total_tests": total_tests,
            "total_operations": total_ops,
            "avg_ops_per_second": avg_ops_per_second / len(self.results) if self.results else 0
        }
        
        # ç”Ÿæˆå»ºè®®
        report["recommendations"] = self._generate_recommendations()
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†ææ•°æ®åº“æ€§èƒ½
        if 'database' in self.results:
            db_results = self.results['database']
            avg_db_ops = statistics.mean([r.ops_per_second for r in db_results]) if db_results else 0
            
            if avg_db_ops < 100:
                recommendations.append("æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç´¢å¼•é…ç½®å’ŒæŸ¥è¯¢ä¼˜åŒ–")
            
            high_error_tests = [r for r in db_results if r.error_count > 0]
            if high_error_tests:
                recommendations.append("æ•°æ®åº“æ“ä½œå­˜åœ¨é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥é…ç½®å’ŒæŸ¥è¯¢è¯­æ³•")
        
        # åˆ†æç¼“å­˜æ€§èƒ½
        if 'cache' in self.results:
            cache_results = self.results['cache']
            avg_cache_ops = statistics.mean([r.ops_per_second for r in cache_results]) if cache_results else 0
            
            if avg_cache_ops < 1000:
                recommendations.append("ç¼“å­˜æ“ä½œæ€§èƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥Redisé…ç½®å’Œç½‘ç»œå»¶è¿Ÿ")
            
            high_memory_tests = [r for r in cache_results if r.memory_usage_mb > 100]
            if high_memory_tests:
                recommendations.append("ç¼“å­˜æ“ä½œå†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®ç»“æ„å’Œç¼“å­˜ç­–ç•¥")
        
        # åˆ†æWebSocketæ€§èƒ½
        if 'websocket' in self.results:
            ws_results = self.results['websocket']
            avg_ws_ops = statistics.mean([r.ops_per_second for r in ws_results]) if ws_results else 0
            
            if avg_ws_ops < 500:
                recommendations.append("WebSocketæ¶ˆæ¯å¤„ç†æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®å¯ç”¨å‹ç¼©å’Œæ‰¹é‡ä¼ è¾“")
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        
        return recommendations
    
    def save_report(self, filename: str = None) -> str:
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_report_{timestamp}.json"
        
        report = self.generate_report()
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
            return filename
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            raise


# ä¾¿æ·å‡½æ•°
async def run_performance_benchmarks(
    database_url: str = "postgresql://stockschool:stockschool123@localhost:15432/stockschool",
    save_report: bool = True
) -> Dict[str, Any]:
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    benchmark = IntegratedBenchmark(database_url)
    
    try:
        await benchmark.run_all_benchmarks()
        report = benchmark.generate_report()
        
        if save_report:
            benchmark.save_report()
        
        return report
        
    except Exception as e:
        logging.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        raise


# ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ å¼€å§‹ç›‘æ§ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    try:
        report = await run_performance_benchmarks()
        
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•æ‘˜è¦:")
        print(f"æ€»æµ‹è¯•ç±»åˆ«: {report['summary']['overall']['total_test_categories']}")
        print(f"æ€»æµ‹è¯•æ•°é‡: {report['summary']['overall']['total_tests']}")
        print(f"æ€»æ“ä½œæ•°é‡: {report['summary']['overall']['total_operations']}")
        print(f"å¹³å‡æ“ä½œ/ç§’: {report['summary']['overall']['avg_ops_per_second']:.2f}")
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, recommendation in enumerate(report['recommendations'], 1):
            print(f"{i}. {recommendation}")
        
        print("\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())